from utils.general import get_dataset, get_sample
from utils.nlp import get_similar_word_pairs
import os
import pandas as pd
from pathlib import Path
from tqdm import trange, tqdm
from utils.test_utils import ConfCreator
import matplotlib.pyplot as plt
import pickle
import gensim
import numpy as np

PROJECT_DIR = Path(__file__).parent.parent
MODELS_DIR = os.path.join(PROJECT_DIR, 'results', 'models')
RESULTS_DIR = os.path.join(PROJECT_DIR, 'results', 'gradient_analysis')
FAST_TEXT_PATH = os.path.join(PROJECT_DIR, 'data', 'wiki-news-300d-1M.vec', 'wiki-news-300d-1M.vec')


def get_use_case_entity_pairs(conf, sampler_conf):
    dataset = get_dataset(conf)

    complete_sampler_conf = sampler_conf.copy()
    complete_sampler_conf['permute'] = conf['permute']
    sample = get_sample(dataset, complete_sampler_conf)

    return sample


def get_similar_word_pairs_grads(conf, sampler_conf, grad_conf, sim_type, sim_metric, sim_thrs, sim_op_eq,
                                 sem_emb_model=None, continuous_res=False):
    tok = conf['tok']
    size = sampler_conf['size']
    fine_tune = 'simple'
    grad_text_unit = grad_conf['text_unit']
    grad_special_tokens = grad_conf['special_tokens']

    grads = []
    for uc in conf['use_case']:
        print("\n\n", uc)
        uc_conf = conf.copy()
        uc_conf['use_case'] = uc

        # Get data
        encoded_dataset = get_use_case_entity_pairs(conf=uc_conf, sampler_conf=sampler_conf)

        # Get similar words
        pair_of_entities = [(row[0], row[1]) for row in encoded_dataset]
        similar_word_pair_map = get_similar_word_pairs(pair_of_entities=pair_of_entities, sim_type=sim_type,
                                                       metric=sim_metric, thrs=sim_thrs, op_eq=sim_op_eq,
                                                       sem_emb_model=sem_emb_model, continuous_res=continuous_res)

        # Get precomputed word gradients generated by an EM-fine-tuned BERT model
        grad_path = f"{uc}_{tok}_{size}_{fine_tune}_{grad_text_unit}_{grad_special_tokens}"
        grad_path = os.path.join(RESULTS_DIR, uc, grad_path)
        uc_grad = pickle.load(open(f"{grad_path}.pkl", "rb"))
        grad_data = []
        for g in uc_grad:
            grad_data.append({
                'left_words': [x[2:] for x in g['grad']['left']],
                'right_words': [x[2:] for x in g['grad']['right']],
                'left_grads': g['grad']['left_grad']['sum'],
                'right_grads': g['grad']['right_grad']['sum'],
            })

        # Get word pair embeddings and measure if they encode some similarity knowledge
        # Loop over the tested word pair similarity thresholds
        for thr in similar_word_pair_map:
            print(f"THR: {thr}")
            similar_word_pairs = similar_word_pair_map[thr]
            num_all_pairs = similar_word_pairs['num_all_pairs']
            sem_model_sims = [s for pair_sims in similar_word_pairs['sims'] for s in pair_sims]

            if len(similar_word_pairs['idxs']) > 0:

                pair_grads = []
                labels = []
                skips = []
                it = 0
                for i in trange(len(similar_word_pairs['idxs'])):
                    record_id = similar_word_pairs['idxs'][i]
                    record_grads = grad_data[record_id]
                    grad_left_words = [x.replace('.0', '') for x in record_grads['left_words']]
                    grad_right_words = [x.replace('.0', '') for x in record_grads['right_words']]
                    record_pairs = similar_word_pairs['pairs'][i]

                    for k in range(len(record_pairs)):
                        pair = (str(record_pairs[k][0]), str(record_pairs[k][1]))

                        if pair[0] not in grad_left_words or pair[1] not in grad_right_words:
                            # print(pair[0])
                            # print(grad_left_words)
                            # print(pair[1])
                            # print(grad_right_words)
                            skips.append(it)
                            it += 1
                            continue

                        left_word_idx = grad_left_words.index(pair[0])
                        right_word_idx = grad_right_words.index(pair[1])

                        pair_grad = record_grads['left_grads'][left_word_idx] + record_grads['right_grads'][
                            right_word_idx]
                        pair_grads.append(pair_grad)
                        labels.append(encoded_dataset[record_id][2]['labels'].item())

                        it += 1

                assert len(pair_grads) == len([sem_model_sims[x] for x in range(len(sem_model_sims)) if x not in skips])
                print(f"\tSKIPS: {len(skips)}")

                grads.append({
                    'use_case': uc,
                    'thr': thr,
                    'grads': pair_grads,
                    'tot_sim_word_pairs': np.sum([len(x) for x in similar_word_pairs['pairs']]),
                    'tot_pairs': num_all_pairs,
                    'sim_pairs_coverage': (np.sum([len(x) for x in similar_word_pairs['pairs']]) / num_all_pairs) * 100,
                    'dataset_coverage': (len(similar_word_pairs) / len(pair_of_entities)) * 100,
                    'sem_model_sims': [sem_model_sims[x] for x in range(len(sem_model_sims)) if x not in skips],
                    'labels': labels,
                    'skips': len(skips)
                })

            else:
                grads.append({
                    'use_case': uc,
                    'thr': thr,
                    'grads': None,
                    'tot_sim_word_pairs': 0,
                    'tot_pairs': num_all_pairs,
                    'sim_pairs_coverage': None,
                    'dataset_coverage': None,
                    'sem_model_sims': None,
                    'labels': None,
                    'skips': None
                })

    return grads


def load_results(res_type, res_metric):

    with open(os.path.join(RESULTS_DIR, f'bert_{res_type}_grad_{res_metric}.pkl'), 'rb') as f:
        res = pickle.load(f)

    res_tab = pd.DataFrame(res)
    res_tab['use_case'] = res_tab['use_case'].map(use_case_map)

    return res_tab


def plot_continuous_results(results, res_type, save_path=None):
    ncols = 4
    nrows = 3
    figsize = (18, 8)

    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, sharey=True, sharex=True)
    axes = axes.flat

    # loop over the use cases
    for idx, use_case in enumerate(use_case_map.values()):
        ax = axes[idx]
        uc_results = results[results['use_case'] == use_case]

        sem_model_sims = np.array(uc_results["sem_model_sims"].values[0])
        grads = np.array(uc_results["grads"].values[0])
        labels = np.array(uc_results['labels'].values[0])
        match_labels = labels == 1
        ax.scatter(x=grads, y=sem_model_sims, alpha=1, color='tab:blue')
        # ax.scatter(x=grads[match_labels], y=sem_model_sims[match_labels], alpha=0.3, color='tab:orange')
        # ax.scatter(x=grads[~match_labels], y=sem_model_sims[~match_labels], alpha=0.3, color='tab:blue')

        ax.set_title(use_case, fontsize=16)

        if idx % ncols == 0:
            if res_type == 'syntax':
                ylabel = 'Jaccard sim.'
            elif res_type == 'semantic':
                ylabel = 'FastText cosine sim.'
            else:
                raise NotImplementedError()

            ax.set_ylabel(ylabel, fontsize=16)

        if idx // ncols == nrows - 1:
            ax.set_xlabel('Gradients', fontsize=16)
        ax.xaxis.set_tick_params(labelsize=14)
        ax.yaxis.set_tick_params(labelsize=14)
        # start, end = ax.get_ylim()
        # ax.yaxis.set_ticks(np.arange(start, end, 25))

    # handles, labels = ax.get_legend_handles_labels()
    # fig.legend(handles, labels, bbox_to_anchor=(.58, 0.01), ncol=2, fontsize=16)

    plt.tight_layout()
    plt.subplots_adjust(wspace=0.1, hspace=0.2)

    if save_path:
        plt.savefig(save_path, bbox_inches='tight')

    plt.show()


if __name__ == '__main__':
    use_cases = ["Structured_Fodors-Zagats", "Structured_DBLP-GoogleScholar", "Structured_DBLP-ACM",
                 "Structured_Amazon-Google", "Structured_Walmart-Amazon", "Structured_Beer",
                 "Structured_iTunes-Amazon", "Textual_Abt-Buy", "Dirty_iTunes-Amazon", "Dirty_DBLP-ACM",
                 "Dirty_DBLP-GoogleScholar", "Dirty_Walmart-Amazon"]

    conf = {
        'use_case': use_cases,
        'data_type': 'train',  # 'train', 'test', 'valid'
        'model_name': 'bert-base-uncased',
        'tok': 'sent_pair',  # 'sent_pair', 'attr', 'attr_pair'
        'label_col': 'label',
        'left_prefix': 'left_',
        'right_prefix': 'right_',
        'max_len': 128,
        'permute': False,
        'verbose': False,
        'return_offset': False,
    }

    sampler_conf = {
        'size': None,
        'target_class': 'both',  # 'both', 0, 1
        'seeds': [42, 42],  # [42 -> class 0, 42 -> class 1]
    }

    grad_conf = {
        'text_unit': 'words',
        'special_tokens': True,
        'agg': None,  # 'mean'
        'agg_target_cat': ['all', 'all_pos', 'all_neg', 'all_pred_pos', 'all_pred_neg', 'tp', 'tn', 'fp', 'fn']
    }

    use_case_map = ConfCreator().use_case_map

    sim_type = 'semantic'
    compute = True

    if sim_type == 'syntax':
        sim_metric = 'jaccard'

        if compute is True:
            syntactic_knowledge = get_similar_word_pairs_grads(conf, sampler_conf, grad_conf, sim_type=sim_type,
                                                               sim_metric=sim_metric, sim_thrs=[0.7], sim_op_eq=False,
                                                               continuous_res=True)
            out_fname = os.path.join(RESULTS_DIR, f'bert_{sim_type}_grad_{sim_metric}.pkl')

            with open(out_fname, 'wb') as f:
                pickle.dump(syntactic_knowledge, f)

        else:
            res = load_results(res_type=sim_type, res_metric=sim_metric)
            plot_save_path = os.path.join(RESULTS_DIR, f'PLOT_bert_{sim_type}_grad.png')
            plot_continuous_results(res, sim_type, save_path=plot_save_path)

    elif sim_type == 'semantic':
        sim_metric = 'cosine'

        if compute is True:
            fasttext_model = gensim.models.KeyedVectors.load_word2vec_format(FAST_TEXT_PATH, binary=False,
                                                                             encoding='utf8')
            semantic_knowledge = get_similar_word_pairs_grads(conf, sampler_conf, grad_conf, sim_type=sim_type,
                                                              sim_metric=sim_metric, sim_thrs=[0.7], sim_op_eq=False,
                                                              sem_emb_model=fasttext_model, continuous_res=True)
            out_fname = os.path.join(RESULTS_DIR, f'bert_{sim_type}_grad_{sim_metric}.pkl')

            with open(out_fname, 'wb') as f:
                pickle.dump(semantic_knowledge, f)

        else:
            res = load_results(res_type=sim_type, res_metric=sim_metric)
            plot_save_path = os.path.join(RESULTS_DIR, f'PLOT_bert_{sim_type}_grad.png')
            plot_continuous_results(res, sim_type, save_path=plot_save_path)

    print(":)")
