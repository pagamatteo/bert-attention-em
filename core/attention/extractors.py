import pandas as pd
import numpy as np
import torch
from core.data_models.em_dataset import EMDataset
from tqdm import tqdm
import unicodedata
from utils.bert_utils import tokenize_entity_pair


class AttentionExtractor(object):
    """
    This class extracts the attention maps generated by the input model on the
    provided data.
    The following parameters are returned for each record of the dataset:
    - attns: the attention maps
    - preds: the prediction generated by the model
    - tokens: the record word pieces
    - all the other parameters provided in the record features
    """

    def __init__(self, dataset: EMDataset, model, **kwargs):

        assert isinstance(dataset, EMDataset), "Wrong data type for parameter 'dataset'."

        self.dataset = dataset
        self.tokenizer = dataset.tokenizer
        self.model = model
        self.special_tokens = True
        if kwargs is not None:
            if 'special_tokens' in kwargs:
                assert isinstance(kwargs['special_tokens'], bool)
                self.special_tokens = kwargs['special_tokens']

        self.model.eval()

    def __len__(self):
        return len(self.dataset)

    @staticmethod
    def check_attn_features(attn_features: tuple):
        EMDataset.check_features(attn_features)
        err_msg = "Wrong attention features format."
        params = ['tokens', 'attns', 'preds']
        f = attn_features[2]
        assert all([p in f for p in params]), err_msg
        assert isinstance(f['tokens'], list), err_msg
        if f['attns'] is not None:
            assert isinstance(f['attns'], (tuple, np.ndarray)), err_msg
        if f['preds'] is not None:
            assert isinstance(f['preds'], torch.Tensor), err_msg

    @staticmethod
    def check_batch_attn_features(batch_attn_features: list):
        assert isinstance(batch_attn_features, list), "Wrong data type for parameter 'batch_attn_features'."
        assert len(batch_attn_features) > 0, "Empty attention features."

        for attn_features in batch_attn_features:
            AttentionExtractor.check_attn_features(attn_features)

    def __getitem__(self, idx):

        left_entity, right_entity, features = self.dataset[idx]

        assert isinstance(left_entity, pd.Series), "Wrong data type for parameter 'left_entity'."
        assert isinstance(right_entity, pd.Series), "Wrong data type for parameter 'right_entity'."
        assert 'input_ids' in features, "'input_ids' not found."
        assert 'attention_mask' in features, "'attention_mask' not found."
        assert 'token_type_ids' in features, "'token_type_ids' not found."

        input_ids = features['input_ids'].unsqueeze(0)
        attention_mask = features['attention_mask'].unsqueeze(0)
        token_type_ids = features['token_type_ids'].unsqueeze(0)

        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask,
                                 token_type_ids=token_type_ids)
        if 'logits' not in outputs:
            preds = None
        else:
            logits = outputs['logits']
            preds = torch.argmax(logits, axis=1)
        attns = outputs['attentions']

        # # TODO: add residual
        # attns_with_residual = []
        # for layer_attns in attns:
        #     # add residual connections by summing the attention weights on the main diagonal with 1
        #     layer_attns_with_residual = layer_attns[0, :, :] + np.eye(layer_attns.shape[-1])[None, ...]
        #     # normalize the attention weights along the column dimension (where softmax is applied)
        #     layer_attns_with_residual /= layer_attns_with_residual.sum(axis=-1)[..., None]
        #     attns_with_residual.append(layer_attns_with_residual[None, ...])
        # attns = tuple(attns_with_residual)

        # param = {}
        # # remove useless features and preserve other features
        # useless_features = ['input_ids']
        # for f in features:
        #     if f not in useless_features:
        #         param[f] = features[f]

        param = features.copy()
        param["tokens"] = self.tokenizer.convert_ids_to_tokens(input_ids[0])
        param["attns"] = attns
        param["preds"] = preds

        return left_entity, right_entity, param

    def extract_all(self):
        attn_features = []
        for features in tqdm(self):
            attn_features.append(features)

        return attn_features


class WordAttentionExtractor(AttentionExtractor):
    """
    This class extract word-level attention.
    """

    def __init__(self, dataset: EMDataset, model, **kwargs):

        super().__init__(dataset, model)
        self.dataset_len = len(dataset)
        self.tokenizer = dataset.tokenizer
        self.max_len = dataset.max_len
        self.tokenization = dataset.tokenization
        self.special_tokens = False
        self.agg_metric = 'mean'
        self.available_agg_metrics = ['mean', 'max']
        if kwargs is not None:
            if 'special_tokens' in kwargs:
                assert isinstance(kwargs['special_tokens'], bool)
                self.special_tokens = kwargs['special_tokens']

            if 'agg_metric' in kwargs:
                assert isinstance(kwargs['agg_metric'], str)
                assert kwargs['agg_metric'] in self.available_agg_metrics
                self.agg_metric = kwargs['agg_metric']

    @staticmethod
    def check_attn_features(attn_features: tuple):
        AttentionExtractor.check_attn_features(attn_features)
        err_msg = "Wrong word attention features format."
        params = ['text_units']
        f = attn_features[2]
        assert all([p in f for p in params]), err_msg
        assert isinstance(f['text_units'], list), err_msg

    @staticmethod
    def check_batch_attn_features(batch_attn_features: list):
        assert isinstance(batch_attn_features, list), "Wrong data type for parameter 'batch_attn_features'."
        assert len(batch_attn_features) > 0, "Empty attention features."

        for attn_features in batch_attn_features:
            WordAttentionExtractor.check_attn_features(attn_features)

    def _get_head_word_attn(self, head_attn: torch.Tensor, left_word_idxs: list, right_word_idxs: list):

        assert isinstance(head_attn, torch.Tensor)
        assert head_attn.ndim == 2
        assert isinstance(left_word_idxs, list)
        assert isinstance(right_word_idxs, list)
        assert len(left_word_idxs) + len(right_word_idxs) <= head_attn.shape[0] - 3  # -3 for one [CLS] and two [SEP]

        # if self.special_tokens:
        #     word_idxs = [(0, 1)] + left_word_idxs + [(left_word_idxs[-1][1], left_word_idxs[-1][1] + 1)]
        #     word_idxs += right_word_idxs + [(right_word_idxs[-1][1], right_word_idxs[-1][1] + 1)]
        # else:
        #     word_idxs = left_word_idxs + right_word_idxs
        word_idxs = left_word_idxs + right_word_idxs

        head_attn = head_attn.detach().numpy()

        # head_attn (n x n) -> softmax applied at column level
        # assert int(round(head_attn.sum(1).sum())) == head_attn.shape[0]

        # sum attention scores by word along the columns in order to obtain for each
        # token its word attentions
        word_to_token_attn = np.empty((head_attn.shape[0], len(word_idxs)))
        for idx, (word_start, word_end) in enumerate(word_idxs):
            word_to_token_attn[:, idx] = head_attn[:, word_start:word_end].sum(1)

        # normalize the attention scores by columns to make them to sum to 1 (as the
        # original softmax)
        min_by_cols = word_to_token_attn.min(1).reshape((-1, 1))
        max_by_cols = word_to_token_attn.max(1).reshape((-1, 1))
        word_to_token_attn = (word_to_token_attn - min_by_cols) / (max_by_cols - min_by_cols)
        word_to_token_attn /= word_to_token_attn.sum(1).reshape((-1, 1))

        # assert int(round(word_to_token_attn.sum(1).sum())) == word_to_token_attn.shape[0]

        # average the attention scores that refer to the same word along the rows in
        # order to obtain for each word its word attentions
        word_to_word_attn = np.empty((len(word_idxs), len(word_idxs)))
        for idx, (word_start, word_end) in enumerate(word_idxs):
            if self.agg_metric == 'mean':
                word_to_word_attn[idx, :] = word_to_token_attn[word_start:word_end, :].mean(0)
            else:
                word_to_word_attn[idx, :] = word_to_token_attn[word_start:word_end, :].max(0)

        # normalize the attention scores by columns to make them to sum to 1 (as the
        # original softmax)
        min_by_cols = word_to_word_attn.min(1).reshape((-1, 1))
        max_by_cols = word_to_word_attn.max(1).reshape((-1, 1))
        word_to_word_attn = (word_to_word_attn - min_by_cols) / (max_by_cols - min_by_cols)
        word_to_word_attn /= word_to_word_attn.sum(1).reshape((-1, 1))

        # assert int(round(word_to_word_attn.sum(1).sum())) == word_to_word_attn.shape[0]

        return word_to_word_attn

    def _get_sent_word_idxs(self, offsets: list, sent):

        assert isinstance(offsets, list), "Wrong data type for parameter 'offsets'."
        assert len(offsets) > 0, "No offsets provided."

        # aggregate all tokens of the sentence that refer to the same word
        # these tokens can be detected by searching for adjacent offsets from the
        # `offset_mapping` parameter
        tokens_to_sent_offsets = offsets[:]
        tokens_by_word = []  # this list will aggregate the token offsets by word
        prec_token_offsets = None
        tokens_in_word = []  # this list will accumulate all the tokens that refer to a target word
        words_offsets = []  # this list will store for each word the range of token idxs that refer to it
        for ix, token_offsets in enumerate(tokens_to_sent_offsets):

            # special tokens (e.g., [CLS], [SEP]) do not refer to any words
            # their offsets are equal to (0, 0)
            if token_offsets == [0, 0] or (sent[token_offsets[0]: token_offsets[1]] == '[SEP]' and self.special_tokens is False):

                # save all the tokens that refer to the previous word
                if len(tokens_in_word) > 0:
                    l = int(np.sum([len(x) for x in tokens_by_word]))
                    words_offsets.append((l, l + len(tokens_in_word)))
                    tokens_by_word.append(tokens_in_word)
                    prec_token_offsets = None
                    tokens_in_word = []

                l = int(np.sum([len(x) for x in tokens_by_word]))
                # words_offsets.append((l, l + 1))
                tokens_by_word.append([token_offsets])
                continue

            if prec_token_offsets is None:
                tokens_in_word.append(token_offsets)
            else:
                # if the offsets of the current and previous tokens are adjacent then they
                # refer to the same word
                if prec_token_offsets[1] == token_offsets[0]:
                    tokens_in_word.append(token_offsets)
                else:
                    # the current token refers to a new word

                    # save all the tokens that refer to the previous word
                    l = int(np.sum([len(x) for x in tokens_by_word]))
                    words_offsets.append((l, l + len(tokens_in_word)))
                    tokens_by_word.append(tokens_in_word)

                    tokens_in_word = [token_offsets]

            prec_token_offsets = token_offsets

        # Note that 'words_offsets' contains only real word offsets, i.e. offsets
        # for special tokens (e.g., [CLS], [SEP], [PAD]), except for the [UNK]
        # token, are omitted

        return words_offsets

    def _get_pair_sent_word_idxs(self, encoded_pair_sent, sent1, sent2):

        assert 'offset_mapping' in encoded_pair_sent, "'encoded_pair_sent' doesn't include the 'offset_mapping' param."

        # split the offset mappings at sentence level by exploiting the [SEP] which
        # is identified with the offsets [0, 0] (as any other special tokens)
        offsets = encoded_pair_sent['offset_mapping'].squeeze(0).tolist()
        sep_idx = offsets[1:].index([0, 0])  # ignore the [CLS] token at the index 0
        left_offsets = offsets[:sep_idx + 2]
        right_offsets = offsets[sep_idx + 1:]

        left_word_idxs = self._get_sent_word_idxs(left_offsets, sent1)
        if self.special_tokens:
            left_word_idxs = [(0, 1)] + left_word_idxs + [(left_word_idxs[-1][1], left_word_idxs[-1][1] + 1)]
        right_word_idxs = self._get_sent_word_idxs(right_offsets, sent2)
        right_word_idxs = [(item[0] + sep_idx + 1, item[1] + sep_idx + 1) for item in right_word_idxs]
        if self.special_tokens:
            right_word_idxs += [(right_word_idxs[-1][1], right_word_idxs[-1][1] + 1)]

        return left_word_idxs, right_word_idxs

    def _get_sent_pair_idxs(self, left_entity, right_entity, tokens):
        def check_word_idxs_consistency(sent1, sent2, tokens, left_word_idxs, right_word_idxs):

            def strip_accents(s):
                return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

            word_idxs = [left_word_idxs, right_word_idxs]

            if self.special_tokens:
                sent1 = f'[CLS] {sent1} [SEP]'
                sent2 = f'{sent2} [SEP]'

            else:
                sent1 = sent1.replace("[SEP] ", "")
                sent2 = sent2.replace("[SEP] ", "")

            truncation = False
            if len(sent1.split()) + len(sent2.split()) > len(left_word_idxs) + len(right_word_idxs):    # truncation
                truncation = True
                # if self.special_tokens:
                #     sent1 = ' '.join(sent1.split()[:len(left_word_idxs) - 2])
                #     sent2 = ' '.join(sent2.split()[:len(right_word_idxs) - 1])
                # else:
                sent1 = ' '.join(sent1.split()[:len(left_word_idxs)])
                sent2 = ' '.join(sent2.split()[:len(right_word_idxs)])

            original_sent1 = strip_accents(sent1).lower().replace("[unk]", "[UNK]").replace("[sep]", "[SEP]").replace("[cls]", "[CLS]")
            original_sent2 = strip_accents(sent2).lower().replace("[unk]", "[UNK]").replace("[sep]", "[SEP]").replace("[cls]", "[CLS]")
            restore_sents = ["", ""]
            for r_idx in range(len(word_idxs)):
                for r in word_idxs[r_idx]:
                    item = ""
                    for a in tokens[r[0]:r[1]]:
                        if a == '#':
                            item += a
                        else:
                            item += a.replace("#", "")
                    # if item != '[CLS]' and item != '[SEP]':
                    restore_sents[r_idx] += f'{item} '
            restore_sents[0] = restore_sents[0][:-1]
            restore_sents[1] = restore_sents[1][:-1]
            if truncation:
                if restore_sents[0].endswith('[SEP]') and not original_sent1.endswith('[SEP]'):
                    restore_sents[0] = restore_sents[0][:-6]
                if restore_sents[1].endswith('[SEP]') and not original_sent2.endswith('[SEP]'):
                    restore_sents[1] = restore_sents[1][:-6]
            first_left = original_sent1
            first_right = original_sent2
            attempts = 0
            num_attempts = 15
            while original_sent1 != restore_sents[0] and original_sent2 != restore_sents[1]:
                print(f"TRUNCATION#{attempts}")
                if original_sent1 != restore_sents[0]:
                    original_sent1 = original_sent1[:-1]
                if original_sent2 != restore_sents[1]:
                    original_sent2 = original_sent2[:-1]
                attempts += 1

                if attempts == num_attempts:
                    break

            if attempts == num_attempts:
                print("LEFT")
                print(original_sent1)
                print(first_left)
                print(restore_sents[0])
                print("RIGHT")
                print(original_sent2)
                print(first_right)
                print(restore_sents[1])
                raise ValueError("Original sentences and restore sentences don't match.")
            print()

            # if self.special_tokens:
            #     restore_sents[0] = f'[CLS] {restore_sents[0]} [SEP]'
            #     restore_sents[1] = f'{restore_sents[1]} [SEP]'

            return restore_sents[0].split(), restore_sents[1].split(), truncation

            # word_idxs = [left_word_idxs, right_word_idxs]
            # if len(sent1.split()) + len(sent2.split()) > len(left_word_idxs) + len(right_word_idxs):
            #     if self.special_tokens:
            #         sent1 = ' '.join(sent1.split()[:len(left_word_idxs) - 2])
            #         sent2 = ' '.join(sent2.split()[:len(right_word_idxs) - 1])
            #     else:
            #         sent1 = ' '.join(sent1.split()[:len(left_word_idxs)])
            #         sent2 = ' '.join(sent2.split()[:len(right_word_idxs)])
            #
            # original_sent1 = strip_accents(sent1).lower().replace("[unk]", "[UNK]")
            # original_sent2 = strip_accents(sent2).lower().replace("[unk]", "[UNK]")
            # restore_sents = ["", ""]
            # for r_idx in range(len(word_idxs)):
            #     for r in word_idxs[r_idx]:
            #         item = ""
            #         for a in tokens[r[0]:r[1]]:
            #             if a == '#':
            #                 item += a
            #             else:
            #                 item += a.replace("#", "")
            #         if item != '[CLS]' and item != '[SEP]':
            #             restore_sents[r_idx] += f'{item} '
            # restore_sents[0] = restore_sents[0][:-1]
            # restore_sents[1] = restore_sents[1][:-1]
            # first_left = original_sent1
            # first_right = original_sent2
            # attempts = 0
            # num_attempts = 15
            # while original_sent1 != restore_sents[0] and original_sent2 != restore_sents[1]:
            #     print(f"TRUNCATION#{attempts}")
            #     if original_sent1 != restore_sents[0]:
            #         original_sent1 = original_sent1[:-1]
            #     if original_sent2 != restore_sents[1]:
            #         original_sent2 = original_sent2[:-1]
            #     attempts += 1
            #
            #     if attempts == num_attempts:
            #         break
            #
            # if attempts == num_attempts:
            #     print("LEFT")
            #     print(original_sent1)
            #     print(first_left)
            #     print(restore_sents[0])
            #     print("RIGHT")
            #     print(original_sent2)
            #     print(first_right)
            #     print(restore_sents[1])
            #     raise ValueError("Original sentences and restore sentences don't match.")
            # print()
            #
            # if self.special_tokens:
            #     restore_sents[0] = f'[CLS] {restore_sents[0]} [SEP]'
            #     restore_sents[1] = f'{restore_sents[1]} [SEP]'
            #
            # return restore_sents[0].split(), restore_sents[1].split()

        flat_features = tokenize_entity_pair(left_entity, right_entity, self.tokenizer, self.tokenization, self.max_len,
                                             return_offset=True)
        sent1 = flat_features['sent1']
        sent2 = flat_features['sent2']
        encoded = {}
        for f in flat_features:
            if f not in ['sent1', 'sent2']:
                encoded[f] = flat_features[f].unsqueeze(0)

        # sent1 = ""
        # sent2 = ""
        # for attr, attr_val in left_entity.iteritems():
        #     sent1 += "{} ".format(str(attr_val))
        # for attr, attr_val in right_entity.iteritems():
        #     sent2 += "{} ".format(str(attr_val))
        # sent1 = sent1[:-1]
        # sent2 = sent2[:-1]
        # # n_words = len(sent1.split()) + len(sent2.split())
        #
        # encoded = self.tokenizer(sent1, sent2, padding='max_length', truncation=True,
        #                          return_tensors="pt", max_length=self.max_len,
        #                          add_special_tokens=True, pad_to_max_length=True,
        #                          return_attention_mask=False,
        #                          return_offsets_mapping=True)

        left_word_idxs, right_word_idxs = self._get_pair_sent_word_idxs(encoded, sent1, sent2)

        if self.special_tokens:
            sent1 = f'[CLS] {sent1} [SEP]'
            sent2 = f'{sent2} [SEP]'

        else:
            sent1 = sent1.replace("[SEP] ", "")
            sent2 = sent2.replace("[SEP] ", "")

        truncation = False
        if len(sent1.split()) + len(sent2.split()) > len(left_word_idxs) + len(right_word_idxs):  # truncation
            truncation = True

        # left_out_words, right_out_words, truncation = check_word_idxs_consistency(sent1, sent2, tokens, left_word_idxs,
        #                                                               right_word_idxs)

        # if self.special_tokens:
        #     out_words = ['[CLS]'] + left_out_words + ['[SEP]'] + right_out_words + ['[SEP]']
        # else:
        #     out_words = left_out_words + right_out_words
        # out_words = left_out_words + right_out_words
        out_words = sent1.split() + sent2.split()

        return left_word_idxs, right_word_idxs, out_words, truncation

    def _get_word_attn(self, left_entity: pd.Series, right_entity: pd.Series,
                       features: dict):

        # check data types
        assert isinstance(left_entity, pd.Series)
        assert isinstance(right_entity, pd.Series)
        assert isinstance(features, dict)

        # features should contain the attention maps, the preds and the tokens
        assert "attns" in features
        assert "preds" in features
        assert "tokens" in features

        attns = features["attns"]
        n_layers = len(attns)
        n_heads = attns[0].shape[1]

        if self.tokenization in ['sent_pair', 'attr_pair']:
            left_word_idxs, right_word_idxs, out_words, truncation = self._get_sent_pair_idxs(left_entity, right_entity,
                                                                                              features['tokens'])

        else:
            raise NotImplementedError()

        if not truncation:
            word_attns = np.empty((n_layers, n_heads, len(out_words), len(out_words)))
            for layer in range(n_layers):
                heads = attns[layer].squeeze(0)
                for head in range(n_heads):
                    head_attn = heads[head]
                    word_attns[layer, head, :, :] = self._get_head_word_attn(head_attn, left_word_idxs, right_word_idxs)

            assert word_attns.shape[2] == len(out_words)

        else:
            word_attns = None

        # override word-piece-level with word-level attention maps
        features['attns'] = word_attns
        features['text_units'] = out_words

        return left_entity, right_entity, features

    def __len__(self):
        return self.dataset_len

    def __getitem__(self, idx):
        left_entity, right_entity, features = super().__getitem__(idx)
        return self._get_word_attn(left_entity, right_entity, features)

    def extract(self, idx):
        return self[idx]

    def extract_all(self):

        attr_features = []
        for features in tqdm(self):
            attr_features.append(features)

        return attr_features


class AttributeAttentionExtractor(AttentionExtractor):
    """
    This class extracts attribute-level attention maps.
    """

    def __init__(self, dataset: EMDataset, model, **kwargs):

        super().__init__(dataset, model)
        self.dataset_len = len(dataset)
        self.tokenizer = dataset.tokenizer
        self.max_len = dataset.max_len
        self.attrs = dataset.columns
        self.tokenization = dataset.tokenization
        self.invalid_attr_attn_maps = 0
        self.special_tokens = False
        self.agg_metric = 'mean'
        self.available_agg_metrics = ['mean', 'max']
        if kwargs is not None:
            if 'special_tokens' in kwargs:
                assert isinstance(kwargs['special_tokens'], bool)
                self.special_tokens = kwargs['special_tokens']

            if 'agg_metric' in kwargs:
                assert isinstance(kwargs['agg_metric'], str)
                assert kwargs['agg_metric'] in self.available_agg_metrics
                self.agg_metric = kwargs['agg_metric']

    @staticmethod
    def check_attn_features(attn_features: tuple):
        AttentionExtractor.check_attn_features(attn_features)
        err_msg = "Wrong attribute attention features format."
        params = ['text_units']
        f = attn_features[2]
        assert all([p in f for p in params]), err_msg
        assert isinstance(f['text_units'], list), err_msg

    @staticmethod
    def check_batch_attn_features(batch_attn_features: list):
        assert isinstance(batch_attn_features, list), "Wrong data type for parameter 'batch_attn_features'."
        assert len(batch_attn_features) > 0, "Empty attention features."

        for attn_features in batch_attn_features:
            AttributeAttentionExtractor.check_attn_features(attn_features)

    def _get_head_attr_attn(self, head_attn: torch.Tensor, left_idxs: list,
                            right_idxs: list):

        assert isinstance(head_attn, torch.Tensor), "Wrong data type for parameter 'head_attn'."
        assert head_attn.ndim == 2, "Attention map is not bi-dimensional"
        assert isinstance(left_idxs, list), "Wrong data type for parameter 'left_idxs'."
        assert isinstance(right_idxs, list), "Wrong data type for parameter 'right_idxs'."

        all_idxs = left_idxs + right_idxs
        head_attn = head_attn.detach().numpy()

        # head_attn (n x n) -> softmax applied at column level
        # assert int(round(head_attn.sum(1).sum())) == head_attn.shape[0]

        # sum attention scores by attribute along the columns in order to obtain for
        # each token its attribute attentions
        attr_to_word_attn = np.empty((head_attn.shape[0], len(all_idxs)))
        for idx, (attr_start, attr_end) in enumerate(all_idxs):
            attr_to_word_attn[:, idx] = head_attn[:, attr_start:attr_end].sum(1)

        # normalize the attention scores by columns to make them to sum to 1 (as the
        # original softmax)
        min_by_cols = attr_to_word_attn.min(1).reshape((-1, 1))
        max_by_cols = attr_to_word_attn.max(1).reshape((-1, 1))
        attr_to_word_attn = (attr_to_word_attn - min_by_cols) / (max_by_cols - min_by_cols)
        attr_to_word_attn /= attr_to_word_attn.sum(1).reshape((-1, 1))

        # assert int(round(attr_to_word_attn.sum(1).sum())) == attr_to_word_attn.shape[0]
        # average the attention scores that refer to the same attribute along the
        # rows in order to obtain for each attribute its attribute attentions
        attr_to_attr_attn = np.empty((len(all_idxs), len(all_idxs)))
        for idx, (attr_start, attr_end) in enumerate(all_idxs):
            assert len(attr_to_word_attn[attr_start:attr_end, :]) > 0
            if self.agg_metric == 'mean':
                attr_to_attr_attn[idx, :] = attr_to_word_attn[attr_start:attr_end, :].mean(0)
            else:
                attr_to_attr_attn[idx, :] = attr_to_word_attn[attr_start:attr_end, :].max(0)

        # normalize the attention scores by columns to make them to sum to 1 (as the
        # original softmax)
        min_by_cols = attr_to_attr_attn.min(1).reshape((-1, 1))
        max_by_cols = attr_to_attr_attn.max(1).reshape((-1, 1))
        attr_to_attr_attn = (attr_to_attr_attn - min_by_cols) / (max_by_cols - min_by_cols)
        attr_to_attr_attn /= attr_to_attr_attn.sum(1).reshape((-1, 1))

        # assert int(round(attr_to_attr_attn.sum(1).sum())) == attr_to_attr_attn.shape[0]

        return attr_to_attr_attn

    def _get_sent_word_idxs(self, offsets: list):

        assert isinstance(offsets, list), "Wrong data type for parameter 'offsets'."
        assert len(offsets) > 0, "No offsets provided."

        # aggregate all tokens of the sentence that refer to the same word
        # these tokens can be detected by searching for adjacent offsets from the
        # `offset_mapping` parameter
        tokens_to_sent_offsets = offsets[:]
        tokens_by_word = []  # this list will aggregate the token offsets by word
        prec_token_offsets = None
        tokens_in_word = []  # this list will accumulate all the tokens that refer to a target word
        words_offsets = []  # this list will store for each word the range of token idxs that refer to it
        for ix, token_offsets in enumerate(tokens_to_sent_offsets):

            # special tokens (e.g., [CLS], [SEP]) do not refer to any words
            # their offsets are equal to (0, 0)
            if token_offsets == [0, 0]:

                # save all the tokens that refer to the previous word
                if len(tokens_in_word) > 0:
                    l = int(np.sum([len(x) for x in tokens_by_word]))
                    words_offsets.append((l, l + len(tokens_in_word)))
                    tokens_by_word.append(tokens_in_word)
                    prec_token_offsets = None
                    tokens_in_word = []

                l = int(np.sum([len(x) for x in tokens_by_word]))
                # words_offsets.append((l, l + 1))
                tokens_by_word.append([token_offsets])
                continue

            if prec_token_offsets is None:
                tokens_in_word.append(token_offsets)
            else:
                # if the offsets of the current and previous tokens are adjacent then they
                # refer to the same word
                if prec_token_offsets[1] == token_offsets[0]:
                    tokens_in_word.append(token_offsets)
                else:
                    # the current token refers to a new word

                    # save all the tokens that refer to the previous word
                    l = int(np.sum([len(x) for x in tokens_by_word]))
                    words_offsets.append((l, l + len(tokens_in_word)))
                    tokens_by_word.append(tokens_in_word)

                    tokens_in_word = [token_offsets]

            prec_token_offsets = token_offsets

        # Note that 'words_offsets' contains only real word offsets, i.e. offsets
        # for special tokens (e.g., [CLS], [SEP], [PAD]), except for the [UNK]
        # token, are omitted

        return words_offsets

    def _get_pair_sent_word_idxs(self, encoded_pair_sent):

        assert 'offset_mapping' in encoded_pair_sent, "'encoded_pair_sent' doesn't include the 'offset_mapping' param."

        # split the offset mappings at sentence level by exploting the [SEP] which
        # is identified with the offsets [0, 0] (as any other special tokens)
        offsets = encoded_pair_sent['offset_mapping'].squeeze(0).tolist()
        sep_idx = offsets[1:].index([0, 0])  # ignore the [CLS] token at the index 0
        left_offsets = offsets[:sep_idx + 2]
        right_offsets = offsets[sep_idx + 1:]

        left_word_idxs = self._get_sent_word_idxs(left_offsets)
        right_word_idxs = self._get_sent_word_idxs(right_offsets)

        return left_word_idxs, right_word_idxs

    def _get_entity_pair_attr_idxs(self, left_entity: pd.Series,
                                   right_entity: pd.Series, features: dict):

        assert isinstance(left_entity, pd.Series), "Wrong data type for parameter 'left_entity'."
        assert isinstance(right_entity, pd.Series), "Wrong data type for parameter 'right_entity'."
        assert isinstance(features, dict), "Wrong data type for parameter 'features'."

        sent1 = ""
        sent2 = ""
        left_attr_len_map = []
        for attr, attr_val in left_entity.iteritems():
            sent1 += "{} ".format(str(attr_val))
            left_attr_len_map.append(len(str(attr_val).split()))
        right_attr_len_map = []
        for attr, attr_val in right_entity.iteritems():
            sent2 += "{} ".format(str(attr_val))
            right_attr_len_map.append(len(str(attr_val).split()))
        sent1 = sent1[:-1]
        sent2 = sent2[:-1]
        n_words = len(sent1.split()) + len(sent2.split())

        encoded = self.tokenizer(sent1, sent2, padding='max_length', truncation=True,
                                 return_tensors="pt", max_length=self.max_len,
                                 add_special_tokens=True, pad_to_max_length=True,
                                 return_attention_mask=False,
                                 return_offsets_mapping=True)

        left_word_idxs, right_word_idxs = self._get_pair_sent_word_idxs(encoded)

        cum_len = 0
        left_attr_idxs = []
        last_left_attr_idx = None
        left_trunc = False
        for left_attr_len in left_attr_len_map:

            if left_trunc:
                left_attr_idxs.append(None)
            else:

                left_attr_words = left_word_idxs[cum_len: cum_len + left_attr_len]
                assert len(left_attr_words) <= left_attr_len
                if len(left_attr_words) < left_attr_len or len(left_attr_words) == 0:
                    left_trunc = True

                if len(left_attr_words) > 0:
                    left_attr_start_idx = left_attr_words[0][0]
                    left_attr_end_idx = left_attr_words[-1][1]
                    left_attr_idxs.append((left_attr_start_idx, left_attr_end_idx))
                    last_left_attr_idx = left_attr_end_idx

                    cum_len += left_attr_len
                else:
                    left_attr_idxs.append(None)

        assert last_left_attr_idx is not None

        cum_len = 0
        right_attr_idxs = []
        right_trunc = False
        for iix, right_attr_len in enumerate(right_attr_len_map):

            if right_trunc:
                right_attr_idxs.append(None)
            else:

                right_attr_words = right_word_idxs[cum_len: cum_len + right_attr_len]
                assert len(right_attr_words) <= right_attr_len
                if len(right_attr_words) < right_attr_len or len(right_attr_words) == 0:
                    right_trunc = True

                if len(right_attr_words) > 0:
                    right_attr_start_idx = right_attr_words[0][0]
                    right_attr_end_idx = right_attr_words[-1][1]
                    right_attr_idxs.append((right_attr_start_idx + last_left_attr_idx,
                                            right_attr_end_idx + last_left_attr_idx))

                    cum_len += right_attr_len
                else:
                    right_attr_idxs.append(None)

        return left_attr_idxs, right_attr_idxs

    def _check_attr_idxs_consistency(self, left_idxs, right_idxs, tokens):

        left_trunc = False
        right_trunc = False
        assert len(left_idxs) == len(self.attrs)
        assert len(right_idxs) == len(self.attrs)
        assert left_idxs[0][0] == 1  # ignore [CLS] token

        # check left attribute indexes consistency
        sep_idx = tokens.index('[SEP]')
        last_left_valid_attr = None
        idx = 0
        while last_left_valid_attr is None:
            last_left_valid_attr = left_idxs[-(1 + idx)]
            idx += 1
        assert last_left_valid_attr[1] == sep_idx
        if idx > 1:
            left_trunc = True

        assert last_left_valid_attr[1] + 1 == right_idxs[0][0]  # ignore [SEP] token

        # check right attribute indexes consistency
        last_sep_idx = tokens[sep_idx + 1:].index('[SEP]') + sep_idx + 1
        last_right_valid_attr = None
        idx = 0
        while last_right_valid_attr is None:
            last_right_valid_attr = right_idxs[-(1 + idx)]
            idx += 1
        assert last_right_valid_attr[1] == last_sep_idx
        if idx > 1:
            assert last_right_valid_attr[1] == self.max_len - 1
            right_trunc = True

        return left_trunc or right_trunc

    def _get_attr_idxs(self, num_entity_attrs: int, tokens: np.ndarray,
                       offset=None):
        assert isinstance(num_entity_attrs, int), "Wrong data type for parameter 'num_entity_attrs'."
        assert num_entity_attrs > 0, "Wrong value for parameter 'num_entity_attrs' (>0)."
        assert isinstance(tokens, np.ndarray), "Wrong data type for parameter 'tokens'."
        assert len(tokens) > 0, "No tokens provided: empty array."

        tokens_by_attr = np.where(tokens == '[SEP]')[0]
        if offset is not None:
            tokens_by_attr += offset
        truncation = False
        idxs = None

        if len(tokens_by_attr) < num_entity_attrs:
            truncation = True

            return idxs, truncation

        idxs = []
        if self.special_tokens:
            if offset is None:
                idxs = [(0, 1)]

        if offset is not None:
            start = offset
        else:
            start = 1
        for ix in range(num_entity_attrs):
            idxs.append((start, tokens_by_attr[ix]))
            start = tokens_by_attr[ix]
            if self.special_tokens:
                idxs.append((tokens_by_attr[ix], tokens_by_attr[ix] + 1))
            start += 1

        if np.array([(item[1] - item[0]) == 0 for item in idxs]).sum() > 0:
            truncation = True

        return idxs, truncation

    def _get_attr_pair_idxs(self, left_entity: pd.Series, right_entity: pd.Series,
                            features: dict):
        assert isinstance(left_entity, pd.Series), "Wrong data type for parameter 'left_entity'."
        assert isinstance(right_entity, pd.Series), "Wrong data type for parameter 'right_entity'."
        assert isinstance(features, dict), "Wrong data type for parameter 'features'."
        tokens = features['tokens']
        token_type_ids = features['token_type_ids'].squeeze(0)
        attention_mask = features['attention_mask'].squeeze(0)

        # print([(ix, r) for ix, r in enumerate(tokens)])

        # get the tokens associated to left and right entities
        valid_token_type_ids = token_type_ids[attention_mask == 1]
        assert set(valid_token_type_ids.detach().numpy()) == {0, 1}
        sent_split_idx = np.where(valid_token_type_ids == 1)[0]
        assert isinstance(sent_split_idx, np.ndarray)
        assert len(sent_split_idx) > 0
        sent_split_idx = sent_split_idx[0]
        assert valid_token_type_ids[sent_split_idx:].sum() == len(valid_token_type_ids) - sent_split_idx

        left_tokens = np.array(tokens[:sent_split_idx])
        # right_tokens = np.array(['[CLS]'] + tokens[sent_split_idx:])  # add fake [CLS] token only for alignment
        right_tokens = np.array(tokens[sent_split_idx:])

        # get attribute indexes from the left entity
        left_idxs, left_trunc = self._get_attr_idxs(len(left_entity), left_tokens)
        left_last_idx = None
        if left_idxs is not None:
            left_last_idx = left_idxs[-1][1]
            # if self.special_tokens:
            #    left_last_idx -= 1

        # get attribute indexes from the right entity
        right_idxs, right_trunc = self._get_attr_idxs(len(right_entity),
                                                      right_tokens,
                                                      offset=left_last_idx)

        return left_idxs, right_idxs, left_trunc or right_trunc

    def _get_attr_attn(self, left_entity: pd.Series, right_entity: pd.Series,
                       features: dict):

        # check data types
        assert isinstance(left_entity, pd.Series), "Wrong data type for parameter 'left_entity'."
        assert isinstance(right_entity, pd.Series), "Wrong data type for parameter 'right_entity'."
        assert isinstance(features, dict), "Wrong data type for parameter 'features'."

        # 'features' should contain the attention maps, the preds and the tokens
        assert "attns" in features, "No 'attns' found in the features."
        assert "preds" in features, "No 'preds' found in the features."
        assert "tokens" in features, "No 'tokens' found in the features."
        assert "token_type_ids" in features, "No 'token_type_ids' found in the features."
        assert 'attention_mask' in features, "No 'attention_mask' found in the features."

        attns = features["attns"]
        n_layers = len(attns)
        n_heads = attns[0].shape[1]
        n_attrs = len(self.attrs) * 2
        out_attrs = self.attrs

        # in order to aggregate the attention scores at attribute level it is needed
        # to know the word pieces that refer to the same attribute

        if self.tokenization == 'sent_pair':
            left_idxs, right_idxs = self._get_entity_pair_attr_idxs(left_entity,
                                                                    right_entity,
                                                                    features)

            # check attribute indexes consistency
            truncation = self._check_attr_idxs_consistency(left_idxs, right_idxs,
                                                           features["tokens"])

            if not truncation and self.special_tokens:
                left_idxs = [(0, 1)] + left_idxs + [(left_idxs[-1][1], left_idxs[-1][1] + 1)]
                right_idxs = right_idxs + [(right_idxs[-1][1], right_idxs[-1][1] + 1)]
                n_attrs += 3  # [CLS] and two [SEP] tokens
                out_attrs = ['[CLS]'] + self.attrs + ['[SEP]']

        elif self.tokenization == 'attr':
            # left_idxs, right_idxs, truncation = self._get_attr_idxs(left_entity,
            #                                                         right_entity,
            #                                                         features)
            idxs, truncation = self._get_attr_idxs(len(left_entity) +
                                                   len(right_entity),
                                                   np.array(features["tokens"]))
            if not truncation:
                left_idxs = idxs[:len(left_entity)]
                right_idxs = idxs[len(left_entity):]
                if self.special_tokens:
                    n_attrs += len(self.attrs) * 2 + 1  # [CLS] + [SEP] between each attribute pair
                    out_attrs = ['[CLS]']
                    for a in self.attrs:
                        out_attrs += [a]
                        out_attrs += ['[SEP]']

        elif self.tokenization == 'attr_pair':
            left_idxs, right_idxs, truncation = self._get_attr_pair_idxs(left_entity,
                                                                         right_entity,
                                                                         features)
            if not truncation and self.special_tokens:
                n_attrs += len(self.attrs) * 2 + 1  # [CLS] + [SEP] between each attribute pair
                out_attrs = ['[CLS]']
                for a in self.attrs:
                    out_attrs += [a]
                    out_attrs += ['[SEP]']

        if truncation:
            # print("TRUNC")
            # print(left_entity.values)
            # print(right_entity.values)
            # print(features["tokens"])

            # some attribute has been truncated, so don't produce the attribute-level
            # attention maps
            features['attns'] = None
            features['text_units'] = out_attrs

            self.invalid_attr_attn_maps += 1

            return left_entity, right_entity, features

        # else:
        #   print("OK")
        #   print(left_entity.values)
        #   print(right_entity.values)
        #   print("LEFT")
        #   for l in left_idxs:
        #     print(features["tokens"][l[0]:l[1]])
        #   print("RIGHT")
        #   for r in right_idxs:
        #     print(features["tokens"][r[0]:r[1]])
        #   print("\n\n")

        assert len(left_idxs) + len(
            right_idxs) == n_attrs, "Attribute indexes length is different from the number of attributes."

        attr_attns = np.empty((n_layers, n_heads, n_attrs, n_attrs))
        for layer in range(n_layers):
            heads = attns[layer].squeeze(0)
            for head in range(n_heads):
                head_attn = heads[head]
                attr_attns[layer, head, :, :] = self._get_head_attr_attn(head_attn,
                                                                         left_idxs,
                                                                         right_idxs)

        # override word-piece-level with attribute-level attention maps
        features['attns'] = attr_attns
        features['text_units'] = out_attrs

        if self.special_tokens:
            assert (attr_attns.shape[2] - 1) // 2 == len(out_attrs) - 1
        else:
            assert attr_attns.shape[2] // 2 == len(out_attrs)

        return left_entity, right_entity, features

    def __len__(self):
        return self.dataset_len

    def __getitem__(self, idx):
        left_entity, right_entity, features = super().__getitem__(idx)

        return self._get_attr_attn(left_entity, right_entity, features)

    def get_num_invalid_attr_attn_maps(self):
        return self.invalid_attr_attn_maps

    def extract(self, idx):
        return self[idx]

    def extract_all(self):

        attr_features = []
        for features in tqdm(self):
            attr_features.append(features)

        return attr_features
